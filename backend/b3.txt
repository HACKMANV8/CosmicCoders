import pandas as pd
from collections import defaultdict
from fastapi import APIRouter, Request, HTTPException
from utils.helpers import find_dataset_path, read_tabular_file

router = APIRouter()

@router.post("/naivebayes")
async def naive_bayes(request: Request):
    body = await request.json()
    dataset_id = body.get("dataset_id")
    params = body.get("params", {})
    target = params.get("target")
    example = params.get("example", {})

    if not dataset_id:
        raise HTTPException(status_code=400, detail="dataset_id is required")
    if not target:
        raise HTTPException(status_code=400, detail="params.target is required")

    csv_path = find_dataset_path(dataset_id)
    df = read_tabular_file(csv_path)
    if target not in df.columns:
        raise HTTPException(status_code=400, detail=f"Target column '{target}' not found in dataset")

    dataset_preview = df.head(5).to_dict(orient="records")
    N = len(df)

    # --- Step 1: Priors ---
    priors = {}
    priors_details = []
    for cls, count in df[target].value_counts().items():
        p = count / N
        priors[cls] = round(p, 6)
        priors_details.append({
            "class": cls,
            "count": int(count),
            "total": int(N),
            "formula": f"P({cls}) = count({cls}) / N = {count} / {N} = {round(p,6)}"
        })

    step1 = {
        "step": 1,
        "title": "Compute Priors",
        "result": {"priors": priors, "details": priors_details},
        "description": "Calculate P(Class) = count(Class)/N"
    }

    # --- Step 2: Conditional Likelihoods ---
    likelihoods = defaultdict(lambda: defaultdict(dict))
    details = []
    for feature in df.columns:
        if feature == target:
            continue
        for cls in df[target].unique():
            subset = df[df[target] == cls]
            total_cls = len(subset)
            for val, count in subset[feature].value_counts().items():
                p = (count + 1) / (total_cls + df[feature].nunique())  # Laplace smoothing
                likelihoods[cls][feature][val] = round(p, 6)
                details.append({
                    "class": cls,
                    "feature": feature,
                    "value": val,
                    "count": int(count),
                    "total_cls": int(total_cls),
                    "formula": f"P({feature}={val}|{cls}) = (count({val},{cls})+1) / ({total_cls}+{df[feature].nunique()}) = {round(p,6)}"
                })

    step2 = {
        "step": 2,
        "title": "Compute Likelihoods with Laplace Smoothing",
        "result": {"details": details},
        "description": "For each feature and class, compute P(Feature=value|Class)"
    }

    # --- Step 3: Unnormalized Posterior ---
    per_class = {}
    for cls in df[target].unique():
        prior = priors[cls]
        prob = prior
        multipliers = []
        for feature, value in example.items():
            p = likelihoods[cls][feature].get(value, 1e-6)
            prob *= p
            multipliers.append({"feature": feature, "value": value, "p": round(p, 6)})
        per_class[cls] = {"unnormalized": round(prob, 8), "multipliers": multipliers}

    step3 = {
        "step": 3,
        "title": "Unnormalized Posterior",
        "result": {"per_class": per_class},
        "description": "Multiply priors and conditionals for each class"
    }

    # --- Step 4: Normalize ---
    total = sum(v["unnormalized"] for v in per_class.values())
    posteriors = {cls: round(v["unnormalized"] / total, 8) for cls, v in per_class.items()}
    step4 = {
        "step": 4,
        "title": "Normalize",
        "vars": {"evidence": round(total, 8)},
        "result": {"posteriors": posteriors},
        "description": "Normalize so probabilities sum to 1"
    }

    # --- Step 5: Prediction ---
    predicted = max(posteriors, key=posteriors.get)
    confidence = posteriors[predicted]
    step5 = {
        "step": 5,
        "title": "Prediction",
        "result": {"predicted": predicted, "confidence": round(confidence, 8)},
        "description": "Choose the class with the highest posterior probability"
    }

    return {
        "dataset_preview": dataset_preview,
        "steps": [step1, step2, step3, step4, step5]
    }
